<?xml version="1.0" encoding="ISO-8859-1"?>
<refentry version="5.0-subset Scilab" xml:id="optim_nelder_mead" xml:lang="en"
          xmlns="http://docbook.org/ns/docbook"
          xmlns:xlink="http://www.w3.org/1999/xlink"
          xmlns:svg="http://www.w3.org/2000/svg"
          xmlns:ns3="http://www.w3.org/1999/xhtml"
          xmlns:mml="http://www.w3.org/1998/Math/MathML"
          xmlns:db="http://docbook.org/ns/docbook">
  <info>
    <pubdate>27-Nov-2006</pubdate>
  </info>

  <refnamediv>
    <refname>optim_nelder_mead</refname>

    <refpurpose>A Nelder &amp; Mead optimization method</refpurpose>
  </refnamediv>

  <refsynopsisdiv>
    <title>Calling Sequence</title>

    <synopsis>[x_opt,x_history] = optim_nelder_mead(f, x0, ItMX, Tol, MaxEvalFunc, Log, kelley_restart, kelley_alpha)</synopsis>
  </refsynopsisdiv>

  <refsection>
    <title>Parameters</title>

    <variablelist>
      <varlistentry>
        <term>f</term>

        <listitem>
          <para>objective function</para>
        </listitem>
      </varlistentry>

      <varlistentry>
        <term>x0</term>

        <listitem>
          <para>initial starting simplex (must be n x n+1 matrix)</para>
        </listitem>
      </varlistentry>

      <varlistentry>
        <term>ItMX</term>

        <listitem>
          <para>the maximum of Nelder &amp; Mead iteration steps (optional
          parameter: ItMX = 100)</para>
        </listitem>
      </varlistentry>

      <varlistentry>
        <term>Tol</term>

        <listitem>
          <para>a tolerance on the value of the objective function between 2
          consecutive iterations (optional parameter: Tol = 0.0)</para>
        </listitem>
      </varlistentry>

      <varlistentry>
        <term>MaxEvalFunc</term>

        <listitem>
          <para>maximum number of objective function evaluation (in 1 Nelder
          &amp; Mead iteration, there are several function evaluation)
          (optional parameter: MaxEvalFunc = 10*ItMX)</para>
        </listitem>
      </varlistentry>

      <varlistentry>
        <term>Log</term>

        <listitem>
          <para>a boolean. If true, displays some informations during the run
          of the optimization (optional parameter: false by default)</para>
        </listitem>
      </varlistentry>

      <varlistentry>
        <term>kelley_restart</term>

        <listitem>
          <para>a boolean. If true, we allow the simplex to be recomputed when
          a threshold has been reached (optional parameter: kelley_restart is
          false by default)</para>
        </listitem>
      </varlistentry>

      <varlistentry>
        <term>kelley_alpha</term>

        <listitem>
          <para>a threshold related to the shape of the simplex (optional
          parameter: kelley_alpha = 1e-4)</para>
        </listitem>
      </varlistentry>

      <varlistentry>
        <term>x_opt</term>

        <listitem>
          <para>the best solution found so far</para>
        </listitem>
      </varlistentry>

      <varlistentry>
        <term>x_history</term>

        <listitem>
          <para>the list of each simplexes tested so far (a list of list of
          n+1 points with n the dimension of the vector of parameters)</para>
        </listitem>
      </varlistentry>
    </variablelist>
  </refsection>

  <refsection>
    <title>Description</title>

    <para>A Nelder &amp; Mead optimization method.</para>
  </refsection>

  <refsection>
    <title>Examples</title>

    <programlisting role="example"><![CDATA[ 
//
// The Rosenbrock function
//
function Res = min_bd_rosenbrock()
  Res = [-2 -2]';
endfunction
function Res = max_bd_rosenbrock()
  Res = [2 2]';
endfunction
function Res = opti_rosenbrock()
  Res = [1 1]';
endfunction
function y = rosenbrock(x)
  y = 100*(x(2)-x(1)^2)^2+(1-x(1))^2;
endfunction

ItMX = 100;
TOL  = 1e-4;
MaxEvalFunc = 400;
Min = min_bd_rosenbrock();
Max = max_bd_rosenbrock();
x_init(:,1) = (Max - Min).*rand(2, 1) + Min;
x_init(:,2) = (Max - Min).*rand(2, 1) + Min;
x_init(:,3) = (Max - Min).*rand(2, 1) + Min;
   
// Start the optimization
printf('Initial iteration\n');
printf('x_init = '); disp(x_init)

// fss is not needed here because the default line search method doesn't need a second derivative function
[x_opt, x_history] = optim_nelder_mead(rosenbrock, x_init, ItMX, TOL, MaxEvalFunc);
printf('xopt = '); disp(x_opt)
printf('f_opt = %f\n', rosenbrock(x_opt));
 ]]></programlisting>
  </refsection>

  <refsection>
    <title>See Also</title>

    <simplelist type="inline">
      <member><link linkend="step_nelder_mead">step_nelder_mead</link></member>
    </simplelist>
  </refsection>

  <refsection>
    <title>Authors</title>

    <variablelist>
      <varlistentry>
        <term>collette</term>

        <listitem>
          <para>Yann COLLETTE (ycollet@freesurf.fr)</para>
        </listitem>
      </varlistentry>
    </variablelist>
  </refsection>
</refentry>
